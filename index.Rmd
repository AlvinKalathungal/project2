---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Name and EID here

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

```{R}
library(tidyverse)
library(dplyr)
library(knitr)
library(ggplot2)
#install.packages("maptools")
library(maptools)
#install.packages("mapdata")
library(mapdata)
#install.packages("ggthemes")
#install.packages("usmap")
library("usmap")
library(ggthemes)
#install.packages("tibble")
library(tibble)
#install.packages("viridis")
library(viridis)
#install.packages("readr")
library(readr)
library(cluster)
```

```{R}
smiteData <- read_csv("SmiteData.csv")
smiteData %>% select(1:4,6,8,10,12,14,16) -> smiteData
names(smiteData)[3] <- 'dmgtype'
names(smiteData)[4] <- 'baseHP'
names(smiteData)[8] <- 'attackPersecond'
names(smiteData)[10] <- 'portPHY'
head(smiteData)

```

### Cluster Analysis

```{R}
library(cluster)

smiteData%>%
    select_if(is.numeric) %>%
    cor(use = "pair") %>%
    as.data.frame %>%
    rownames_to_column("var1") %>%
    pivot_longer(-1, names_to = "var2", values_to = "correlation") ->
    basicStats

basicStats %>%
    ggplot(aes(var1, var2, fill = correlation)) + geom_tile() +
    scale_fill_gradient2(low = "red", mid = "white",
        high = "blue") + geom_text(aes(label = round(correlation,
    2)), color = "black", size = 4) + theme(axis.text.x = element_text(angle = 90,
    hjust = 1)) + coord_fixed()
```

```{r}
sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(smiteData, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot() + geom_line(aes(x = 1:10, y = sil_width)) +
    scale_x_continuous(name = "k", breaks = 1:10)
```
```{r}
smite_pam <- smiteData%>% pam(k=2)
smite_pam

smiteData %>% slice(smite_pam$id.med)
```
```{r}
plot(smite_pam, which = 2)
```

```{r}
library(GGally)
smiteClust <- smiteData %>%
    mutate(cluster = as.factor(smite_pam$clustering))
ggpairs(smiteClust, columns = 3:7, aes(color = cluster))
```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
princomp(smiteData[4:10], cor = TRUE) -> pca1
summary(pca1, loadings = T)
```
```{r}
eigval <- pca1$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC

ggplot() + geom_bar(aes(y=varprop, x=1:7), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:7)) + 
  geom_text(aes(x=1:7, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + scale_x_continuous(breaks=1:10)
```

```{r}
round(cumsum(eigval)/sum(eigval), 2) 
smiteData$`Attack/sec`
```

```{r}
results <-  smiteData %>% as.data.frame %>% mutate(PC1=pca1$scores[, 1], PC2=pca1$scores[, 2], PC3=pca1$scores[, 3], PC4=pca1$scores[, 4])  #add PCs
  
results %>% ggplot(aes(PC1, PC2, color=Range)) + geom_point(size=4)

results %>% ggplot(aes(PC3, PC4, color=Damage)) + geom_point(size=4)
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
smiteData$`Damage Type`
smiteData %>% select(3:10) -> classData
classData <- classData %>% mutate(dmgtype = ifelse(dmgtype=="Physical",1,0))

fit <- glm(dmgtype ~ . , data=classData, family="binomial") 
probs <- predict(fit, type="response")
class_diag(probs, classData$dmgtype, positive="1")
table(truth = classData$dmgtype, predictions = probs>.5)
```
```{r}
 smiteData %>% mutate(dmgtype = ifelse(dmgtype=="Physical",1,0)) %>% select(3:10) ->smitedata2
```

```{R}
set.seed(321)
k = 10

data <- sample_frac(smitedata2)  #randomly order rows
folds <- rep(1:k, length.out = nrow(data))  #create folds

diags <- NULL

i = 1
for (i in 1:k) {
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$dmgtype
    
    fit <- glm(dmgtype ~ . , data=train, family="binomial")
    

    probs <- predict(fit, newdata = test, type = "response")
    
    diags <- rbind(diags, class_diag(probs, truth, positive = "1"))
}

# average performance metrics across all folds
summarize_all(diags, mean)
# cross-validation of linear classifier here
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
fit <- knn3(dmgtype ~ . , data=classData)
probs <- predict(fit, newdata=classData)[,2] #we choose the second column since that's the probability of "True"
class_diag(probs, classData$dmgtype, positive="1") 
table(truth = classData$dmgtype, predictions = probs>.5)
```

```{R}
set.seed(322)
k = 10

data <- sample_frac(smitedata2) 
folds <- rep(1:k, length.out = nrow(data))  

diags <- NULL

i = 1
for (i in 1:k) {
    # create training and test sets
    train <- data[folds != i, ]
    test <- data[folds == i, ]
    truth <- test$dmgtype
    
    # train model
    fit <- knn3(dmgtype ~ . , data = train)  ### SPECIFY THE KNN MODEL FIT TO THE TRAINING SET HERE
    
    # test model
    probs <- predict(fit, newdata = test)[, 2]
    ### GET PREDICTIONS FROM THE TRAINED MODEL ON THE TEST SET HERE
    
    # get performance metrics for each fold
    diags <- rbind(diags, class_diag(probs, truth, positive = "1"))
}

# average performance metrics across all folds
summarize_all(diags, mean)

```

Discussion


### Regression/Numeric Prediction

```{R}
fit<- lm(baseHP~., data=smiteData)
yhat<-predict(fit) 
mean((smiteData$baseHP-yhat)^2)
```

```{R}
set.seed(322)
k=2 #choose number of folds
data<-smitedata2[sample(nrow(smitedata2)),] #randomly order rows
folds<-cut(seq(1:nrow(smitedata2)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
## Fit linear regression model to training set
fit<-lm(baseHP~.,data=train)
## Get predictions/y-hats on test set (fold i)
yhat<-predict(fit,newdata=test)
## Compute prediction error (MSE) for fold i
diags<-mean((test$baseHP-yhat)^2)
}
mean(diags)
```
```{r}
library(rpart); library(rpart.plot)

fit <- train(Speed ~ . , data=smitedata2, method="rpart")
rpart.plot(fit$finalModel,digits=4)

```

Discussion

### Python 

```{R}
library(reticulate)
```

```{python}
# python code here
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




